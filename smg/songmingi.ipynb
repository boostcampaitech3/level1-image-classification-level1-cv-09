{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Mingi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torch import randperm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import csv\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root: str = '~/', \\\n",
    "                transforms = None, \\\n",
    "                val_ratio:float = 0.0):\n",
    "        \n",
    "        self.path = os.path.join(root, 'input/data/train/images')\n",
    "        self.transforms = transforms\n",
    "        self.val_ratio = val_ratio\n",
    "        self.dirs = next(os.walk(self.path))[1]\n",
    "        self.images = {dir:[] for dir in self.dirs}\n",
    "        for dir in self.dirs:\n",
    "            dir_path = os.path.join(self.path ,dir)\n",
    "            files = next(os.walk(dir_path))[2]\n",
    "            files = [file for file in files if not file.startswith('._')]\n",
    "            self.images[dir] = files\n",
    "        self.classes = {'Mask': {'Wear': 0, 'Incorrect':1, 'Not Wear':2},\n",
    "                'Gender': {'Male':0, 'Female':1},\n",
    "                'Age': lambda x:x//30}\n",
    "\n",
    "        self.data = self.get_data()\n",
    "        self.write_csv()\n",
    "\n",
    "\n",
    "    def get_label(self, dirname: str, filename:str) -> int:\n",
    "        idx, gender, race, age = dirname.split('_')\n",
    "        mask = ''\n",
    "        if filename.startswith('normal'):\n",
    "            mask = 'Not Wear'\n",
    "        elif filename.startswith('incorrect_mask'):\n",
    "            mask = 'Incorrect'\n",
    "        else:\n",
    "            mask = 'Wear'\n",
    "\n",
    "        if gender == 'male':\n",
    "            gender = 'Male'\n",
    "        elif gender == 'female':\n",
    "            gender = 'Female'\n",
    "\n",
    "        label_num = self.classes['Mask'][mask] * 6 + self.classes['Gender'][gender] * 3 + self.classes['Age'](int(age))\n",
    "        return label_num\n",
    "\n",
    "    \n",
    "    def write_csv(self):\n",
    "        f = open('data.csv', 'w')\n",
    "        wr = csv.writer(f)\n",
    "        \n",
    "        wr.writerow(('idx', 'dir', 'name', 'label'))\n",
    "\n",
    "        for row in self.data:\n",
    "            wr.writerow(row)\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "    \n",
    "    def get_data(self):\n",
    "        data = []\n",
    "        idx = 0\n",
    "        for dir in self.dirs:\n",
    "            for file in self.images[dir]:\n",
    "                data.append([idx, dir, file, self.get_label(dir, file)])\n",
    "                idx += 1\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def read_images(self):\n",
    "        images = {}\n",
    "        for dir in self.dirs:\n",
    "            dir_path = os.path.join(self.path ,dir)\n",
    "            files = next(os.walk(dir_path))[2]\n",
    "            files = [file for file in files if not file.startswith('._')]\n",
    "            for file in files:\n",
    "                images[dir] = {}\n",
    "                images[dir][file] = {'label': self.get_label(dir, file)}\n",
    "        return images\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        _, dir, name, label = self.data[idx]\n",
    "        img_path = os.path.join(self.path, dir, name)\n",
    "        img = PIL.Image.open(img_path)\n",
    "        if self.transforms:\n",
    "            img = transforms(img)\n",
    "        return img, label\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '*' * 20 + '\\n' + \\\n",
    "                'MyDataset class\\n' + \\\n",
    "                f'path: {self.path}\\n' + \\\n",
    "                f'transforms: {self.transforms}\\n' + \\\n",
    "                f'dirs: {self.dirs[:30]}' + ('...\\n' if len(self.dirs) > 30 else '\\n') + \\\n",
    "                f'len: {len(self)}\\n' + \\\n",
    "                '*' * 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/opt/ml'\n",
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(256)])\n",
    "val_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "MyDataset class\n",
      "path: /opt/ml/input/data/train/images\n",
      "transforms: Compose(\n",
      "    ToTensor()\n",
      "    CenterCrop(size=(256, 256))\n",
      ")\n",
      "dirs: ['006108_male_Asian_19', '000262_male_Asian_56', '005107_female_Asian_38', '001439_male_Asian_58', '001047_male_Asian_60', '006753_male_Asian_19', '003619_male_Asian_57', '006452_female_Asian_18', '001362_female_Asian_28', '000750_female_Asian_52', '001725_female_Asian_25', '000334_female_Asian_52', '006093_male_Asian_19', '001495-1_male_Asian_21', '001282_female_Asian_18', '003004_female_Asian_19', '006164_female_Asian_19', '006102_male_Asian_18', '004242_female_Asian_53', '005530_female_Asian_19', '005028_male_Asian_52', '005051_female_Asian_26', '000661_male_Asian_55', '000827_female_Asian_55', '001479_female_Asian_57', '006075_male_Asian_18', '003307_female_Asian_19', '001454_female_Asian_49', '000270_female_Asian_49', '001019_male_Asian_29']...\n",
      "len: 18900\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "ds = MyDataset(root, transforms, val_ratio)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  (7): Linear(in_features=1000, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg19\n",
    "from torch.nn.modules.linear import Linear\n",
    "\n",
    "model = vgg19(pretrained=True)\n",
    "last_output_features = model.classifier[-1].out_features\n",
    "model.classifier.add_module(str(len(model.classifier)), Linear(in_features=last_output_features, out_features=18))\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train_model(model, dataloader, device, criterion, optimizer, num_epochs):\n",
    "    best_model = None\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('=' * 20)\n",
    "        print(f'Epoch: {epoch}/{num_epochs}')\n",
    "\n",
    "        running_loss = {'train': 0, 'val': 0}\n",
    "        running_corrects = {'train': 0, 'val': 0}\n",
    "\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = criterion(outs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss['train'] += loss.item() * inputs.size(0)\n",
    "            running_corrects['train'] += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss['train'] / len(dl.dataset)\n",
    "        train_acc = running_corrects['train'] / len(dl.dataset)\n",
    "        # val_loss = running_loss[mode] / len(dl.dataset.indicies['val'])\n",
    "        # val_acc = running_corrects[mode] / len(dl.dataset.indicies['val'])\n",
    "\n",
    "        print(f'train loss: {train_loss}, train acc: {train_acc}')\n",
    "        # print(f'val loss: {val_loss}, val acc: {val_acc}')\n",
    "        wandb.log({\"train\": {\"loss\": train_loss, \"acc\": train_acc}})\n",
    "        # wandb.log({\"val\": {\"loss\": val_loss, \"acc\": val_acc}})\n",
    "\n",
    "        if best_acc < train_acc:\n",
    "            train_acc = best_acc\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "model.cuda()\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vdoz6nx6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10092... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc862c1ae2804c2f99d39c255562b9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">smart-feather-14</strong>: <a href=\"https://wandb.ai/songmingi/my%20project/runs/vdoz6nx6\" target=\"_blank\">https://wandb.ai/songmingi/my%20project/runs/vdoz6nx6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220224_042457-vdoz6nx6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vdoz6nx6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "2022-02-24 04:25:41.285632: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-24 04:25:41.285672: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/songmingi/my%20project/runs/12irbc30\" target=\"_blank\">playful-glade-15</a></strong> to <a href=\"https://wandb.ai/songmingi/my%20project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Epoch: 1/3\n",
      "train loss: 0.9573197424664068, train acc: 0.6842328310012817\n",
      "====================\n",
      "Epoch: 2/3\n",
      "train loss: 0.4755650469991896, train acc: 0.8419047594070435\n",
      "====================\n",
      "Epoch: 3/3\n",
      "train loss: 0.3478045103474269, train acc: 0.880740761756897\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init(project=\"my project\", entity=\"songmingi\")\n",
    "best_model = train_model(model, dl, device, criterion, optimizer_ft, 3)\n",
    "torch.save(model, '/opt/ml/model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval images number: 12600\n"
     ]
    }
   ],
   "source": [
    "eval_path = '/opt/ml/input/data/eval/images'\n",
    "eval_images = next(os.walk(eval_path))[2]\n",
    "eval_images = [image for image in eval_images if not image.startswith('._')]\n",
    "\n",
    "print(f'eval images number: {len(eval_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n"
     ]
    }
   ],
   "source": [
    "img_path = os.path.join(eval_path, eval_images[0])\n",
    "img = PIL.Image.open(img_path)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/opt/ml/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "model.eval()\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
